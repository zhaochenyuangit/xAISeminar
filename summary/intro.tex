\section{Introduction}
 People tend to ask themselves "what-if" questions to dream about another better results when unfortunate happens. These "what-if" arguments are usually a (slightly) different world in which a slight change in action may amend the current outcome. Counterfactual explanations (CF) are mostly applied for binary automated decision in social critical domains (such as disease diagnose, loan approval, school admission\dots), to audit possible bias and artifacts. \cite{watcher2017} has pointed out three usages of a explanation of machine decisions: (a.) to understand the decision, (b.) to contest a decision, and (c.) to obtain instructions for better outcomes in the future. Counterfactual explanation, though ignoring the working principle of the model, satisfies all these demands.

 Without "opening the black box", a CF algorithm generates a positive classified user-case based on a originally negative one (or vice versa). Providing that the algorithm suggests a higher GPA score or more deposit in bank account, the user understands the reason of the rejection, but also knows the measure to adopt for the desired outcome. However, if the algorithm suggests a change in the race or gender, which implies that the decision is made through a discrimination factor, the user is expected to contest the unfair decision.

 In practical, there are still several features to taken into consideration. Sparsity, to ensure an actionable advice for the user, the generation should prefer as few changes in items as possible, changing one item vastly is better than changing several slightly. Diversity, to provide a list of choices, the algorithm should offer multiple CF cases varying in the changed item and in different extend. And finally, interpretability, the suggested CF case should be a possible case in the real life. Later, it will be leveraged that sparsity is contradictable with diversity and interpretability, optimizing sparsity usually causes downgrade in other 2 features.


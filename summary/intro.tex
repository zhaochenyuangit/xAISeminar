\section{Introduction}
%先介绍大局观
Machine learning turns out to outperform the conventional expert systems in automated decision especially when it handles large-scale, high-dimensional data. In spite of the convenience, the introducing of machine learning has raised many concerns about the transparency and fairness of the decision. The reason for such concerns is that, once trained, even the designer of the model is not capable to fully interpret it. Obscurity in the decision is not tolerable when the decision could impact humans, for example credit lending, university approval and medical treatment \cite{CFReview}. Therefore, bringing machine learning into social sensitive domains demands a good explanation over the internal working principle of the model (explainability) or the effect of external feature changes on the model (interpretability) \cite{XAIBook}.
Researching of these mechanisms is called explainable AI (XAI) and is currently a heated topic. XAI helps human to understand and trust a model.

%CF 在整体中的位置
In the ML family, some models are inherently transparent, others are opaque. Post-hoc (\emph{latin ``after this''}) explanations are used for the opaque models, and can be further divided as model-specific or model-agnostic approaches. A model-agnostic approach doesn't assume the structure of a model, and has four main branches, namely model simplification, feature importance, visualization and local explanation. Local explanation differ from other explanation methods because they only explain a single explanation \cite{CFReview}. Local explanations are further categorized into approximation and example-based approaches. Counterfactual explanation (CF) belongs to the latter. Rather than explain the whole model, example-based approaches focus on explaining a particular outcome \cite{CFReview}. A CF is such an example that is close to the original input, but with a different prediction label. By looking at which features have been changed, human could gain a better understand of a decision.
  
%两个例子，让读者更好地理解什么是CF
CFs are common in our daily life. For example, ``My university application is denied, but what if my GPA is 0.5 better?'' These ``what-if'' arguments are a (slightly) different world in which a change in action may amend the current outcome. In this article, another two examples are given:

\noindent \textbf{Example 1:} \label{a}(derived from Adult-Income dataset) Alice has graduated many years but still in her prime. Recently she is not satisfied with her salary and want to know whether she can increase her salary over \$50,000 by switching her job or improving her education level.

\noindent \textbf{Example 2:} \label{b}(derived from German-Credit dataset) Bob's loan application has been denied, the reason given by the bank is a high credit risk. Bob want to know what can he do to improve his credit and obtain the loan. Could the solution be an increase in the income, or a lower debt?

% CF的应用场景
In technical details, a CF explanation usually omits the internal logic that leads to a decision, but rather concentrates on the external factors \cite{watcher2017}. Hence, CF is born for black-box models, the least model access situation that only the prediction function is available. Despite this, many purposed CF methods facilitates the gradients or even complete model internals \cite{CFReview}. CF explanations are mostly applied in classification decisions, and a majority of them are binary classifications \cite{CFandAE}. CFs are mainly used for low-dimensional tabular data, their application for image data is relative rare \cite{CFandAE}.

CFs are useful explanations though they rarely reveal any internal logic. \citeauthor{watcher2017} \cite{watcher2017} has pointed out three aims of a explanation of machine decisions: (a.) to understand the decision, (b.) to obtain guidance for future actions, and (c.) to contest a decision by pointing out model bias. CF explanations satisfies all these demands. Providing that in example \ref{a} a higher education level is suggested, then Alice understands that low education level is the reason for low income, but also knows that improving educational level is the way to the desired salary. In example \ref{b}, if a change in the race is suggested, which implies that the decision is made through a discrimination factor, then Bob is expected to contest the unfair decision. The two scenarios described are alternatively named ``feasible CF'' and ``contesting CF'', which will be detailed in section \ref{sec:adversarial}.

In the next section, section \ref{sec:generation}, this article starts introducing mathematical models of the nearest CF explanation, with a focus on tabular input data. In section \ref{sec:adversarial}, the main drawback of nearest CF explanation is given, and the focus transit to causal CF explanation. Section \ref{sec:Causality} introduces the method to include causality into the model. Section \ref{sec:Conclude} concludes this article.


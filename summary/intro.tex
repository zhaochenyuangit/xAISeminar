\section{Introduction}
 People usually ask themselves "what-if" questions to illustrate another better world when unfortunate happens. For example, "My loan application is denied, but what if I have \$2000 more salary?" These "what-if" arguments are a (slightly) different world in which a change in action may amend the current outcome.
 
 Similarly, counterfactual (CF) explanation describes how a (slight) change in the input data will bring about a different prediction result. It omits the internal logic that leads to a decision, but rather concentrates on the external factors. Hence, CF is born for black-box models, and is sometimes considered as a model-agnostic method in XAI. CF explanations are mostly applied in classification decisions, and a majority of them are binary classifications (such as disease diagnose, loan approval, school admission\dots). CFs are mainly used for low-dimensional tabular data, their application for image data is relative rare.

 CFs are useful explanations though they do not reveal any internal logic. \cite{watcher2017} has pointed out three aims of a explanation of machine decisions: (a.) to understand the decision, (b.) to obtain guidance for future actions, and (c.) to contest a decision by pointing out model bias. CF explanations satisfies all these demands. Providing that the algorithm suggests a higher GPA or a higher salary to gain an approval, the user immediately understands the reason of the rejection, but also knows the measure to adopt for the desired outcome. However, if the algorithm suggests a change in the race or gender, which implies that the decision is made through a discrimination factor, the user is expected to contest the unfair decision. The two scenarios described are alternatively named "feasible CF" and "contesting CF", which will be detailed in section \ref{sec:adversarial}.  

 In practical, there are still several features to taken into consideration. 
 \begin{itemize}
   \item  Sparsity: to ensure an actionable advice for the user, the generated CF should prefer as few feature changes as possible, so that human user is able to capture the main reason for a decision and have an idea for the future action. Changing one entry vastly is usually better than changing several ones slightly.
   \item  Diversity: to provide different solutions to the same problem, the algorithm should offer multiple CF cases varying in the perturbed entry as well as in different perturbation strength. Instead of one single "most feasible" solution, having a list to choose from probably contains a more feasible solution for the user.
   \item  And finally, interpretability: the suggested CF example should be representative for its class. The CF example should give human an impression that it is correctly classified under the CF class, and the decision should be the same if is made by a human.
 \end{itemize}
In section \ref{sec:generation}, three methods of generating a CF are introduced, with a focus on tabular input data. Section \ref{sec:adversarial} and \ref{sec:Causualty} list two potential issues of CF explanation. Section \ref{sec:Conclude} concludes this article.

